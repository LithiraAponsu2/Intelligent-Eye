{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lithira/Project1/19_06_24_conda/triton_repo\n",
      "build_env.sh  requirements.txt\ttriton_run.ipynb  violation\n",
      "\u001b[01;34m.\u001b[0m\n",
      "├── \u001b[01;32mbuild_env.sh\u001b[0m\n",
      "├── requirements.txt\n",
      "├── triton_run.ipynb\n",
      "└── \u001b[01;34mviolation\u001b[0m\n",
      "    ├── \u001b[01;34m1\u001b[0m\n",
      "    │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "    │   │   └── model.cpython-310.pyc\n",
      "    │   ├── model.py\n",
      "    │   ├── \u001b[01;35mshow.mp4\u001b[0m\n",
      "    │   ├── xgb_model.pkl\n",
      "    │   ├── yolov8x-seg.engine\n",
      "    │   └── yolov8x-seg.pt\n",
      "    └── config.pbtxt\n",
      "\n",
      "3 directories, 10 files\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls\n",
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8f0970455524cc463593908a3734be4cec5537c2b44b378bad516cf778312fdd\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --shm-size=5G --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "-v $PWD:/mnt/data/model_repository \\\n",
    "custom_triton \\\n",
    "tritonserver \\\n",
    "--model-repository=/mnt/data/model_repository \\\n",
    "--log-verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Command Explanation\n",
    "\n",
    "This `docker run` command starts a container with the NVIDIA Triton Inference Server, configured with various options:\n",
    "\n",
    "- `-d`: Runs the container in detached mode, meaning it runs in the background and does not block your terminal.\n",
    "\n",
    "- `--shm-size=5G`: Sets the size of `/dev/shm` (shared memory) to 5 gigabytes. Shared memory is used for inter-process communication and can be necessary for certain applications, like machine learning inference.\n",
    "\n",
    "- `--gpus all`: Grants the container access to all GPUs on the host machine. This is important for running machine learning models that utilize GPU acceleration.\n",
    "\n",
    "- `-p8000:8000 -p8001:8001 -p8002:8002`: Maps ports from the host to the container. This allows you to access services running on these container ports from the corresponding ports on your host machine.\n",
    "   - `8000`: Typically used for HTTP requests to the Triton server.\n",
    "   - `8001`: Typically used for GRPC requests.\n",
    "   - `8002`: Typically used for the server's metrics endpoint.\n",
    "\n",
    "- `-v $PWD/triton_repo:/mnt/data/model_repository`: Mounts the `triton_repo` directory from your current working directory (`$PWD`) on the host to `/mnt/data/model_repository` inside the container. This is where Triton expects to find the model repository.\n",
    "\n",
    "- `nvcr.io/nvidia/tritonserver:24.05-py3`: Specifies the Docker image to use, which is version `24.05-py3` of NVIDIA's Triton Inference Server from NVIDIA's container registry (`nvcr.io`).\n",
    "\n",
    "- `tritonserver`: The command run inside the container to start the Triton Inference Server.\n",
    "\n",
    "- `--model-repository=/mnt/data/model_repository`: Tells Triton where to find the model repository inside the container.\n",
    "\n",
    "- `--log-verbose=1`: Sets the verbosity level of Triton's logging. Level 1 provides a moderate amount of logging detail.\n",
    "\n",
    "This command sets up and runs a Triton Inference Server in a Docker container with access to shared memory, GPUs, and specific ports, using your local model repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf89196247b6d15ae878baa4a71df80543fc304b501ebe91de2a7eb9e51439f9\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --shm-size=5G --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "  -v $PWD:/mnt/data/model_repository \\\n",
    "  --network bridge \\\n",
    "  custom_triton \\\n",
    "  tritonserver \\\n",
    "  --model-repository=/mnt/data/model_repository \\\n",
    "  --log-verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS         PORTS                              NAMES\n",
      "bf89196247b6   custom_triton   \"/opt/nvidia/nvidia_…\"   6 seconds ago   Up 2 seconds   0.0.0.0:8000-8002->8000-8002/tcp   trusting_stonebraker\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'violation', 'model_version': '1', 'outputs': [{'name': 'OUTPUT0', 'datatype': 'INT32', 'shape': [1], 'data': [1]}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8000/v2/models/violation/versions/1/infer\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"name\": \"INPUT0\",\n",
    "      \"shape\": [1, 1],  # Corrected shape\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [\"some_input_data\"]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=payload)\n",
    "\n",
    "print(json.loads(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lithira/Project1/19_06_24_conda_copy_for_15_07_24/triton_repo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26_07_24.txt  prometheus_config  show.mp4\t   triton_run_2.ipynb\n",
      "build_env.sh  requirements.txt\t triton_run.ipynb  violation\n",
      "\u001b[01;34m.\u001b[0m\n",
      "├── 26_07_24.txt\n",
      "├── \u001b[01;32mbuild_env.sh\u001b[0m\n",
      "├── \u001b[01;34mprometheus_config\u001b[0m\n",
      "│   └── prometheus.yml\n",
      "├── requirements.txt\n",
      "├── \u001b[01;35mshow.mp4\u001b[0m\n",
      "├── triton_run.ipynb\n",
      "├── triton_run_2.ipynb\n",
      "└── \u001b[01;34mviolation\u001b[0m\n",
      "    ├── \u001b[01;34m1\u001b[0m\n",
      "    │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "    │   │   └── model.cpython-310.pyc\n",
      "    │   ├── model.py\n",
      "    │   ├── xgb_model.pkl\n",
      "    │   ├── yolov8x-seg.engine\n",
      "    │   └── yolov8x-seg.pt\n",
      "    ├── config.pbtxt\n",
      "    └── \u001b[01;31mpyenv.tar.gz\u001b[0m\n",
      "\n",
      "4 directories, 14 files\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls\n",
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cae4ed38b87d3a6ac6df68f1a37450a49a943f0e953495bb7485cb12b6d2f188\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --shm-size=5G --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "-v $PWD:/mnt/data/model_repository \\\n",
    "custom_triton \\\n",
    "tritonserver \\\n",
    "--model-repository=/mnt/data/model_repository \\\n",
    "--log-verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE           COMMAND                  CREATED              STATUS              PORTS                              NAMES\n",
      "5db2367acefc   custom_triton   \"/opt/nvidia/nvidia_…\"   About a minute ago   Up About a minute   0.0.0.0:8000-8002->8000-8002/tcp   goofy_bohr\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Command Explanation\n",
    "\n",
    "This `docker run` command starts a container with the NVIDIA Triton Inference Server, configured with various options:\n",
    "\n",
    "- `-d`: Runs the container in detached mode, meaning it runs in the background and does not block your terminal.\n",
    "\n",
    "- `--shm-size=5G`: Sets the size of `/dev/shm` (shared memory) to 5 gigabytes. Shared memory is used for inter-process communication and can be necessary for certain applications, like machine learning inference.\n",
    "\n",
    "- `--gpus all`: Grants the container access to all GPUs on the host machine. This is important for running machine learning models that utilize GPU acceleration.\n",
    "\n",
    "- `-p8000:8000 -p8001:8001 -p8002:8002`: Maps ports from the host to the container. This allows you to access services running on these container ports from the corresponding ports on your host machine.\n",
    "   - `8000`: Typically used for HTTP requests to the Triton server.\n",
    "   - `8001`: Typically used for GRPC requests.\n",
    "   - `8002`: Typically used for the server's metrics endpoint.\n",
    "\n",
    "- `-v $PWD/triton_repo:/mnt/data/model_repository`: Mounts the `triton_repo` directory from your current working directory (`$PWD`) on the host to `/mnt/data/model_repository` inside the container. This is where Triton expects to find the model repository.\n",
    "\n",
    "- `nvcr.io/nvidia/tritonserver:24.05-py3`: Specifies the Docker image to use, which is version `24.05-py3` of NVIDIA's Triton Inference Server from NVIDIA's container registry (`nvcr.io`).\n",
    "\n",
    "- `tritonserver`: The command run inside the container to start the Triton Inference Server.\n",
    "\n",
    "- `--model-repository=/mnt/data/model_repository`: Tells Triton where to find the model repository inside the container.\n",
    "\n",
    "- `--log-verbose=1`: Sets the verbosity level of Triton's logging. Level 1 provides a moderate amount of logging detail.\n",
    "\n",
    "This command sets up and runs a Triton Inference Server in a Docker container with access to shared memory, GPUs, and specific ports, using your local model repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345bb785a059010ac6eb3b3de174b71337d67dce47886d458b26a157d61d91f2\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --shm-size=5G --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "  -v $PWD:/mnt/data/model_repository \\\n",
    "  --network bridge \\\n",
    "  -e TZ=Asia/Kolkata \\\n",
    "  custom_triton \\\n",
    "  tritonserver \\\n",
    "  --model-repository=/mnt/data/model_repository \\\n",
    "  --log-verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS                  PORTS                              NAMES\n",
      "e38c8ef488ed   custom_triton   \"/opt/nvidia/nvidia_…\"   4 seconds ago   Up Less than a second   0.0.0.0:8000-8002->8000-8002/tcp   intelligent_satoshi\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'violation', 'model_version': '1', 'outputs': [{'name': 'OUTPUT0', 'datatype': 'INT32', 'shape': [1], 'data': [1]}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "url = \"http://localhost:8000/v2/models/violation/versions/1/infer\"\n",
    "\n",
    "# Read the video file\n",
    "with open(\"show.mp4\", \"rb\") as video_file:\n",
    "    video_bytes = video_file.read()\n",
    "\n",
    "# Encode the video file to base64\n",
    "video_base64 = base64.b64encode(video_bytes).decode('utf-8')\n",
    "\n",
    "# Set the client number (1 or 2)\n",
    "client_number = 1\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT0\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [video_base64]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CLIENT_NUMBER\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"data\": [client_number]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=payload)\n",
    "\n",
    "print(json.loads(response.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab6c272f07a2b34c617bccc1062045e6f14a9b8fb82d1ec3311f81c1d32effea\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --name prometheus \\\n",
    "  -p 9090:9090 \\\n",
    "  -v $(pwd)/../trition-monitoring/prometheus:/etc/prometheus \\\n",
    "  prom/prometheus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f29f0a4e9378388167fb19022024a9f432e81f0a5b77fd02997209b1772e72ed\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --name grafana \\\n",
    "  -p 3000:3000 \\\n",
    "  grafana/grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I understand. If you're starting each container separately rather than using Docker Compose, I'll provide a step-by-step guide for that approach. Here's how to set up Grafana monitoring for your Triton Inference Server using individual Docker containers:\n",
    "\n",
    "1. Create a project directory:\n",
    "\n",
    "```bash\n",
    "mkdir triton-monitoring\n",
    "cd triton-monitoring\n",
    "```\n",
    "\n",
    "2. Start Triton Inference Server:\n",
    "\n",
    "Assuming you already have your Triton container running, make sure it's exposing the metrics port (usually 8002). If not, start it with:\n",
    "\n",
    "```bash\n",
    "docker run --name triton -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "  -v /path/to/your/models:/models \\\n",
    "  nvcr.io/nvidia/tritonserver:22.12-py3 \\\n",
    "  tritonserver --model-repository=/models\n",
    "```\n",
    "\n",
    "3. Set up Prometheus:\n",
    "\n",
    "Create a Prometheus configuration file:\n",
    "\n",
    "```bash\n",
    "mkdir prometheus\n",
    "cat << EOF > prometheus/prometheus.yml\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'triton'\n",
    "    static_configs:\n",
    "      - targets: ['host.docker.internal:8002']\n",
    "EOF\n",
    "```\n",
    "\n",
    "Start Prometheus container:\n",
    "\n",
    "```bash\n",
    "docker run -d --name prometheus \\\n",
    "  -p 9090:9090 \\\n",
    "  -v $(pwd)/prometheus:/etc/prometheus \\\n",
    "  prom/prometheus:v2.42.0\n",
    "```\n",
    "\n",
    "4. Start Grafana:\n",
    "\n",
    "```bash\n",
    "docker run -d --name grafana \\\n",
    "  -p 3000:3000 \\\n",
    "  grafana/grafana:9.3.6\n",
    "```\n",
    "\n",
    "5. Configure Grafana:\n",
    "\n",
    "   a. Access Grafana at http://localhost:3000\n",
    "   b. Log in with default credentials (admin/admin)\n",
    "   c. Change the password when prompted\n",
    "   d. Add Prometheus as a data source:\n",
    "      - Go to Configuration > Data Sources\n",
    "      - Click \"Add data source\"\n",
    "      - Select Prometheus\n",
    "      - Set the URL to http://host.docker.internal:9090\n",
    "      - Click \"Save & Test\"\n",
    "\n",
    "6. Create a dashboard for Triton metrics:\n",
    "\n",
    "   a. Click the \"+\" icon in the sidebar and select \"Dashboard\"\n",
    "   b. Click \"Add a new panel\"\n",
    "   c. In the query editor, select Prometheus as the data source\n",
    "   d. Start adding queries to visualize Triton metrics, such as:\n",
    "      - `triton_memory_used`\n",
    "      - `triton_queue_count`\n",
    "      - `triton_inflight_request_count`\n",
    "   e. Customize your panels and save the dashboard\n",
    "\n",
    "7. (Optional) Import pre-made Triton dashboards:\n",
    "\n",
    "   a. Go to \"Create\" > \"Import\" in the Grafana sidebar\n",
    "   b. Enter the dashboard ID: 12391 for the Triton Inference Server dashboard\n",
    "   c. Select your Prometheus data source\n",
    "   d. Click \"Import\"\n",
    "\n",
    "8. Monitor and analyze your Triton Inference Server using the Grafana dashboards.\n",
    "\n",
    "Additional notes:\n",
    "\n",
    "- The use of `host.docker.internal` in the Prometheus config allows it to reach your host machine. This may need to be adjusted based on your Docker network setup.\n",
    "- If containers can't communicate, you may need to create a Docker network and connect all containers to it:\n",
    "\n",
    "```bash\n",
    "docker network create triton-monitoring\n",
    "docker network connect triton-monitoring triton\n",
    "docker network connect triton-monitoring prometheus\n",
    "docker network connect triton-monitoring grafana\n",
    "```\n",
    "\n",
    "Then update the Prometheus config to use the container name instead of `host.docker.internal`.\n",
    "\n",
    "- Remember to adjust volume mounts and ports as necessary for your specific setup.\n",
    "\n",
    "Would you like me to explain or elaborate on any part of this setup?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flask response: {\n",
      "  \"message\": \"Video successfully received and saved.\"\n",
      "}\n",
      "\n",
      "Triton response: {'model_name': 'violation', 'model_version': '1', 'outputs': [{'name': 'OUTPUT0', 'datatype': 'INT32', 'shape': [1], 'data': [1]}]}\n",
      "Video sent to both Triton and Flask\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import threading\n",
    "\n",
    "def send_to_triton(video_base64, client_number):\n",
    "    url = \"http://localhost:8000/v2/models/violation/versions/1/infer\"\n",
    "    \n",
    "    payload = json.dumps({\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"INPUT0\",\n",
    "                \"shape\": [1],\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": [video_base64]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"CLIENT_NUMBER\",\n",
    "                \"shape\": [1],\n",
    "                \"datatype\": \"INT32\",\n",
    "                \"data\": [client_number]\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, data=payload)\n",
    "    print(\"Triton response:\", json.loads(response.text))\n",
    "\n",
    "def send_to_flask(video_base64):\n",
    "    flask_url = \"http://localhost:5000/receive_video\"\n",
    "    \n",
    "    payload = {\n",
    "        \"video\": video_base64\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(flask_url, headers=headers, json=payload)\n",
    "    print(\"Flask response:\", response.text)\n",
    "\n",
    "# Read the video file\n",
    "with open(\"show.mp4\", \"rb\") as video_file:\n",
    "    video_bytes = video_file.read()\n",
    "\n",
    "# Encode the video file to base64\n",
    "video_base64 = base64.b64encode(video_bytes).decode('utf-8')\n",
    "\n",
    "# Set the client number (1 or 2)\n",
    "client_number = 1\n",
    "\n",
    "# Create threads for sending to Triton and Flask\n",
    "triton_thread = threading.Thread(target=send_to_triton, args=(video_base64, client_number))\n",
    "flask_thread = threading.Thread(target=send_to_flask, args=(video_base64,))\n",
    "\n",
    "# Start both threads\n",
    "triton_thread.start()\n",
    "flask_thread.start()\n",
    "\n",
    "# Wait for both threads to complete\n",
    "triton_thread.join()\n",
    "flask_thread.join()\n",
    "\n",
    "print(\"Video sent to both Triton and Flask\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# frame output docker volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e14e6741f0a79dc88bb11f1869e46197bb3e467df71274488ba6a8b1407cb512\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --shm-size=5G --gpus all \\\n",
    "  -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "  -v $PWD:/mnt/data/model_repository \\\n",
    "  -v ~/saved_frames:/opt/tritonserver/xgb_capt_3 \\\n",
    "  --network bridge \\\n",
    "  -e TZ=Asia/Kolkata \\\n",
    "  custom_triton \\\n",
    "  tritonserver \\\n",
    "  --model-repository=/mnt/data/model_repository \\\n",
    "  --log-verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
